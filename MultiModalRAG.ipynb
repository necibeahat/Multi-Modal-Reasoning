{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Multimodal retrieval augmentation generation (mRAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.10.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "{TODO: Include a paragraph or two explaining what this example demonstrates, who should be interested in it, and what you need to know before you get started.}\n",
    "\n",
    "Learn more about [web-doc-title](linkback-to-webdoc-page). {TODO: if more than one primary feature, add tag/linkback for each one}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to {TODO: Complete the sentence explaining briefly what you will learn from the notebook, such as\n",
    "training, hyperparameter tuning, or serving}:\n",
    "\n",
    "This notebooks performs best;\n",
    "- document have both text & images \n",
    "- tables in the doc are available as images\n",
    "- the images don't require too much context. If the document contain particular domain knowledge, make sure to pass that info in the prompt. \n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- *{TODO: Add high level bullets for the services/resources demonstrated; e.g., Vertex AI Training}*\n",
    "\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- *{TODO: Add high level bullets for the steps of performed in the notebook}*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "{TODO: Include a paragraph with Dataset information and where to obtain it.} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. \n",
    "\n",
    "Please upgrade to the latest GA version of each package; i.e., --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --user google-cloud-aiplatform pymupdf rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## Set up your Google Cloud account and authenticate\n",
    "Follow the instructions to set up your account and autenticate from [Google's tutorials](https://console.cloud.google.com/cloud-resource-manager). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "\n",
    "import sys\n",
    "\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# if not running on Colab, try to get the PROJECT_ID automatically\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    import subprocess\n",
    "\n",
    "    PROJECT_ID = subprocess.check_output(\n",
    "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
    "    ).strip()\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from rich.markdown import Markdown as rich_Markdown\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    HarmCategory,\n",
    "    HarmBlockThreshold,\n",
    "    Image,\n",
    "    Part,\n",
    ")\n",
    "\n",
    "text_model = GenerativeModel(\"gemini-1.5-pro\")\n",
    "multimodal_model = GenerativeModel(\"gemini-1.5-pro-001\")\n",
    "multimodal_model_flash = GenerativeModel(\"gemini-1.5-flash-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "# Instantiate text model with appropriate name and version\n",
    "text_model = GenerativeModel(\"gemini-1.0-pro\")  # works with text, code\n",
    "\n",
    "# Multimodal models: Choose based on your performance/cost needs\n",
    "multimodal_model_15 = GenerativeModel(\n",
    "    \"gemini-1.5-pro-001\"\n",
    ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - complex reasoning\n",
    "\n",
    "# Load text embedding model from pre-trained source\n",
    "text_embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
    "\n",
    "# Load multimodal embedding model from pre-trained source\n",
    "multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
    "    \"multimodalembedding\"\n",
    ")  # works with image, image with caption(~32 words), video, video with caption(~32 words)\n",
    "\n",
    "from multimodal_qa_with_rag_utils import (\n",
    "    get_document_metadata,\n",
    "    set_global_variable,\n",
    ")\n",
    "\n",
    "set_global_variable(\"text_embedding_model\", text_embedding_model)\n",
    "set_global_variable(\"multimodal_embedding_model\", multimodal_embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP1 - Extracting document metadata from both text and images\n",
    "To contextualise the search with the images / documents that are interest to us, we need to extract the metadata from our documents. We will then create embeddings to find similar text or image per our prompgt. In addition, the metadata wil be used for references and citations. It includes essencial elements like page numbers, files manes, images counter, etc. \n",
    "\n",
    "### STEP 1.1 - Generate Embeddings from the metadata\n",
    "Embeddings are used to perform similarity search when querying the data. We are extrating metadata from both images and text as there might be information only given in an image and vice versa. \n",
    "\n",
    "``` get_document_metadata() ``` : extract text & image metadata from the document. Returns two data frames ```text_metadata``` and ```image_metadata```. \n",
    "\n",
    "\n",
    "### STEP 1.2 - Analyse the embeddings \n",
    "Using the functions below, check if the embeddings are correct for your data. \n",
    "- ```text_metadata```:\n",
    "    - ```text```: the original text from the page\n",
    "    - ```text_embedding_page```: the embedding of the original text from the page\n",
    "    - ```chunk_tex```: the original text divided into smaller chunks\n",
    "    - ```chunk_number```: the index of each text chunk\n",
    "    - ```text_embedding_chunk```: the embedding of each text chunk\n",
    "- ```image_metadata```:\n",
    "    - ```img_desc```: Gemini-generated textual description of the image.\n",
    "    - ```mm_embedding_from_text_desc_and_img```: Combined embedding of image and its description, capturing both visual and textual information.\n",
    "    - ```mm_embedding_from_img_only```: Separate text embedding of the generated description, enabling textual analysis and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Specify the PDF folder with multiple PDF ~7m\n",
    "\n",
    "print(\n",
    "    \"Removing pre-exsisting images folder, if any\"\n",
    ")\n",
    "! rm -rf images/\n",
    "\n",
    "pdf_folder_path = \"data/\"  # if running in Vertex AI Workbench.\n",
    "\n",
    "# Specify the image description prompt. Change it\n",
    "# image_description_prompt = \"\"\"Explain what is going on in the image.\n",
    "# If it's a table, extract all elements of the table.\n",
    "# If it's a graph, explain the findings in the graph.\n",
    "# Do not include any numbers that are not mentioned in the image.\n",
    "# \"\"\"\n",
    "\n",
    "image_description_prompt = \"\"\"The interpretation of pharmaceutical information is inherently complex, involving multiple data types such as text, diagrams, tables, and schematics. While large language models (LLMs) demonstrate proficiency in answering straightforward questions, complex inquiries demand an integrated approach to multiple information modalities. This hackathon aims to leverage Google AI's advanced tools and technologies to develop solutions that can effectively interpret and reason with multi-modal data from pharmaceutical annual reports.\n",
    "Participants will work on tasks around complex reasoning such as question answering, information extraction, and data synthesis, showcasing the potential of multi-modal reasoning in the pharmaceutical domain.\n",
    "\n",
    "Important Guidelines:\n",
    "* Prioritize accuracy:  If you are uncertain about any detail, state \"Unknown\" or \"Not visible\" instead of guessing.\n",
    "* Avoid hallucinations: Do not add information that is not directly supported by the image.\n",
    "* Be specific: Use precise language to describe shapes, colors, textures, and any interactions depicted.\n",
    "* Consider context: If the image is a screenshot or contains text, incorporate that information into your description.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Extract text and image metadata from the PDF document\n",
    "text_metadata_df, image_metadata_df = get_document_metadata(\n",
    "    multimodal_model_15,  # we are passing Gemini 1.5 Pro\n",
    "    pdf_folder_path,\n",
    "    image_save_dir=\"images\",\n",
    "    image_description_prompt=image_description_prompt,\n",
    "    embedding_size=1408,\n",
    "    # add_sleep_after_page = True, # Uncomment this if you are running into API quota issues\n",
    "    # sleep_time_after_page = 5,\n",
    "    add_sleep_after_document=True,  # Uncomment this if you are running into API quota issues\n",
    "    sleep_time_after_document=5,  # Increase the value in seconds, if you are still getting quota issues. It will slow down the processing.\n",
    "    # generation_config = # see next cell\n",
    "    # safety_settings =  # see next cell\n",
    ")\n",
    "\n",
    "print(\"\\n\\n --- Completed processing. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Helper Functions\n",
    "\n",
    "```/utils/intro_multimodal_rag_utils.py``` includes the helper functions. \n",
    "\n",
    "- ```get_similar_text_from_query()```: Given a text query, finds text from the document which are relevant, using cosine similarity algorithm. It uses text embeddings from the metadata to compute and the results can be filtered by top score, page/chunk number, or embedding size.\n",
    "- ```print_text_to_text_citation()```: Prints the source (citation) and details of the retrieved text from the get_similar_text_from_query() function.\n",
    "- ```get_similar_image_from_query()```: Given an image path or an image, finds images from the document which are relevant. It uses image embeddings from the metadata.\n",
    "- ```print_text_to_image_citation()```: Prints the source (citation) and the details of retrieved images from the get_similar_image_from_query() function.\n",
    "- ```get_gemini_response()```: Interacts with a Gemini model to answer questions based on a combination of text and image inputs.\n",
    "- ```display_images()```: Displays a series of images provided as paths or PIL Image objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intro_multimodal_rag_utils import (\n",
    "    get_similar_text_from_query,\n",
    "    print_text_to_text_citation,\n",
    "    get_similar_image_from_query,\n",
    "    print_text_to_image_citation,\n",
    "    get_gemini_response,\n",
    "    display_images,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
